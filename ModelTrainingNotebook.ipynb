{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:33.629979Z",
     "iopub.status.busy": "2024-12-08T07:42:33.629645Z",
     "iopub.status.idle": "2024-12-08T07:42:38.221972Z",
     "shell.execute_reply": "2024-12-08T07:42:38.221231Z",
     "shell.execute_reply.started": "2024-12-08T07:42:33.629944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:38.224677Z",
     "iopub.status.busy": "2024-12-08T07:42:38.224273Z",
     "iopub.status.idle": "2024-12-08T07:42:38.291796Z",
     "shell.execute_reply": "2024-12-08T07:42:38.290737Z",
     "shell.execute_reply.started": "2024-12-08T07:42:38.224651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:38.293838Z",
     "iopub.status.busy": "2024-12-08T07:42:38.293410Z",
     "iopub.status.idle": "2024-12-08T07:42:38.314774Z",
     "shell.execute_reply": "2024-12-08T07:42:38.314039Z",
     "shell.execute_reply.started": "2024-12-08T07:42:38.293782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:38.316199Z",
     "iopub.status.busy": "2024-12-08T07:42:38.315935Z",
     "iopub.status.idle": "2024-12-08T07:42:38.328619Z",
     "shell.execute_reply": "2024-12-08T07:42:38.327875Z",
     "shell.execute_reply.started": "2024-12-08T07:42:38.316175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset:load_dataset, language:str):\n",
    "    \"\"\"\n",
    "    Generator function that extracts all sentences in a specified language from a translation dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (load_dataset): A Hugging Face `load_dataset` object representing the dataset containing \n",
    "            translations in different languages.\n",
    "        language (str): The target language for which sentences are to be extracted. This should correspond \n",
    "            to a key in the `translation` field of the dataset.\n",
    "\n",
    "    Yields:\n",
    "        str: Sentences in the specified language from the dataset.\n",
    "\n",
    "    Example:\n",
    "        >>> from datasets import load_dataset\n",
    "        >>> dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
    "        >>> language = \"en\"\n",
    "        >>> sentences = get_all_sentences(dataset['train'], language)\n",
    "        >>> for sentence in list(sentences)[:5]:\n",
    "        ...     print(sentence)\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in dataset:\n",
    "        yield item['translation'][language]\n",
    "\n",
    "def build_tokenizer(config, dataset:load_dataset, language:str) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Builds or loads a tokenizer for a specified language using the Hugging Face `Tokenizers` library.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A configuration dictionary containing the `tokenizer_file` key, which specifies \n",
    "            the file path template for saving/loading the tokenizer. The file path should include a \n",
    "            placeholder for the language.\n",
    "        dataset (load_dataset): A Hugging Face `load_dataset` object representing the dataset containing \n",
    "            translations in different languages.\n",
    "        language (str): The target language for which the tokenizer is being built or loaded. This should \n",
    "            correspond to a key in the `translation` field of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: A `Tokenizer` object built for the specified language.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> from datasets import load_dataset\n",
    "        >>> config = {\"tokenizer_file\": \"tokenizer_{language}.json\"}\n",
    "        >>> dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
    "        >>> language = \"en\"\n",
    "        >>> tokenizer = build_tokenizer(config, dataset['train'], language)\n",
    "        >>> print(tokenizer.get_vocab_size())\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(language))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    print(f\"Tokenizer language : {language} Build Complete.\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:38.329892Z",
     "iopub.status.busy": "2024-12-08T07:42:38.329575Z",
     "iopub.status.idle": "2024-12-08T07:42:49.498599Z",
     "shell.execute_reply": "2024-12-08T07:42:49.497611Z",
     "shell.execute_reply.started": "2024-12-08T07:42:38.329847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cc05b3a1504d2aa8ba33c72ff0d4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8f55c9339f4ad48c1e110d1eebf3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135e141b50b04390ba747aaf303c3bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer language : en Build Complete.\n",
      "Tokenizer language : it Build Complete.\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "tokenizer_src = build_tokenizer(config, raw_dataset, config['lang_src'])\n",
    "tokenizer_tgt = build_tokenizer(config, raw_dataset, config['lang_tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.500722Z",
     "iopub.status.busy": "2024-12-08T07:42:49.499856Z",
     "iopub.status.idle": "2024-12-08T07:42:49.507124Z",
     "shell.execute_reply": "2024-12-08T07:42:49.506032Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.500677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10',\n",
       " 'translation': {'en': '\"Jane, I don\\'t like cavillers or questioners; besides, there is something truly forbidding in a child taking up her elders in that manner.',\n",
       "  'it': '— Jane, non mi piace di essere interrogata. Sta male, del resto, che una bimba tratti così i suoi superiori.'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.511133Z",
     "iopub.status.busy": "2024-12-08T07:42:49.510758Z",
     "iopub.status.idle": "2024-12-08T07:42:49.648212Z",
     "shell.execute_reply": "2024-12-08T07:42:49.647255Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.511096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def causal_mask(size:int)->bool:\n",
    "    \"\"\"\n",
    "    Creates a causal mask for autoregressive models.\n",
    "\n",
    "    Args:\n",
    "        size (int): The size of the square matrix for the mask.\n",
    "\n",
    "    Returns:\n",
    "        bool: A boolean tensor of shape (1, size, size), where the upper triangular part above the diagonal \n",
    "              is masked (False) and the rest is unmasked (True).\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> mask = causal_mask(5)\n",
    "        >>> print(mask)\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch dataset class for bilingual translation tasks.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): A Hugging Face `Dataset` object containing translation data with `translation` fields.\n",
    "        tokenizer_src (Tokenizer): Tokenizer for the source language.\n",
    "        tokenizer_tgt (Tokenizer): Tokenizer for the target language.\n",
    "        src_lang (str): Source language key in the `translation` field of the dataset.\n",
    "        tgt_lang (str): Target language key in the `translation` field of the dataset.\n",
    "        seq_len (int): Fixed sequence length for the input and output tensors.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - \"encoder_input\" (torch.Tensor): Tokenized and padded source sentence (seq_len).\n",
    "            - \"decoder_input\" (torch.Tensor): Tokenized and padded target sentence with `<SOS>` (seq_len).\n",
    "            - \"encoder_mask\" (torch.Tensor): Boolean mask for the encoder input (1, 1, seq_len).\n",
    "            - \"decoder_mask\" (torch.Tensor): Boolean mask for the decoder input (1, seq_len, seq_len).\n",
    "            - \"label\" (torch.Tensor): Tokenized and padded target sentence with `<EOS>` (seq_len).\n",
    "            - \"src_text\" (str): Original source sentence.\n",
    "            - \"tgt_text\" (str): Original target sentence.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> from datasets import load_dataset\n",
    "        >>> dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
    "        >>> src_tokenizer = Tokenizer.from_file(\"tokenizer_en.json\")\n",
    "        >>> tgt_tokenizer = Tokenizer.from_file(\"tokenizer_fr.json\")\n",
    "        >>> bilingual_ds = BilingualDataset(\n",
    "        ...     dataset=dataset['train'],\n",
    "        ...     tokenizer_src=src_tokenizer,\n",
    "        ...     tokenizer_tgt=tgt_tokenizer,\n",
    "        ...     src_lang=\"en\",\n",
    "        ...     tgt_lang=\"fr\",\n",
    "        ...     seq_len=32\n",
    "        ... )\n",
    "        >>> sample = bilingual_ds[0]\n",
    "        >>> print(sample[\"encoder_input\"])\n",
    "        >>> print(sample[\"decoder_input\"])\n",
    "        >>> print(sample[\"label\"])\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset:Dataset, tokenizer_src:Tokenizer, tokenizer_tgt:Tokenizer, src_lang:str, tgt_lang:str, seq_len:int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = dataset\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx)->dict[str:torch.Tensor]:\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.649771Z",
     "iopub.status.busy": "2024-12-08T07:42:49.649416Z",
     "iopub.status.idle": "2024-12-08T07:42:49.690437Z",
     "shell.execute_reply": "2024-12-08T07:42:49.689725Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.649744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(raw_dataset))\n",
    "val_size = len(raw_dataset) - train_size\n",
    "train_ds_raw, val_ds_raw = random_split(raw_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "val_dataset = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.691593Z",
     "iopub.status.busy": "2024-12-08T07:42:49.691354Z",
     "iopub.status.idle": "2024-12-08T07:42:49.697129Z",
     "shell.execute_reply": "2024-12-08T07:42:49.696282Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.691570Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29098"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.698627Z",
     "iopub.status.busy": "2024-12-08T07:42:49.698289Z",
     "iopub.status.idle": "2024-12-08T07:42:49.718679Z",
     "shell.execute_reply": "2024-12-08T07:42:49.717885Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.698591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_random_subset(dataset:Dataset, fraction:float):\n",
    "    \"\"\"\n",
    "    Get a random subset of a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): The original dataset.\n",
    "        fraction (float): Fraction of the dataset to keep (0 < fraction <= 1).\n",
    "    \n",
    "    Returns:\n",
    "        Subset: A smaller dataset containing the random subset.\n",
    "    \"\"\"\n",
    "    assert 0 < fraction <= 1, \"Fraction must be in the range (0, 1].\"\n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    subset_size = int(total_size * fraction)\n",
    "    random_indices = random.sample(range(total_size), subset_size)\n",
    "    return Subset(dataset, random_indices)\n",
    "\n",
    "# Example usage\n",
    "train_fraction = 0.65  # Use 65% of the training dataset\n",
    "val_fraction = 0.65    # Use 65% of the validation dataset\n",
    "\n",
    "train_dataset = get_random_subset(train_dataset, train_fraction)\n",
    "val_dataset = get_random_subset(val_dataset, val_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.720429Z",
     "iopub.status.busy": "2024-12-08T07:42:49.720062Z",
     "iopub.status.idle": "2024-12-08T07:42:49.730199Z",
     "shell.execute_reply": "2024-12-08T07:42:49.729360Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.720389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18913, 2102)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.731586Z",
     "iopub.status.busy": "2024-12-08T07:42:49.731205Z",
     "iopub.status.idle": "2024-12-08T07:42:49.801531Z",
     "shell.execute_reply": "2024-12-08T07:42:49.800507Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.731559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input': tensor([   2,  540,  123,   40, 1646,   76,    3,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1]),\n",
       " 'decoder_input': tensor([    2, 16053,  1603,     5,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0]]], dtype=torch.int32),\n",
       " 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.int32),\n",
       " 'label': tensor([16053,  1603,     5,     3,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'src_text': \"One must have confidence.'\",\n",
       " 'tgt_text': 'Abbi fiducia.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:49.803121Z",
     "iopub.status.busy": "2024-12-08T07:42:49.802715Z",
     "iopub.status.idle": "2024-12-08T07:42:53.592282Z",
     "shell.execute_reply": "2024-12-08T07:42:53.591376Z",
     "shell.execute_reply.started": "2024-12-08T07:42:49.803087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    }
   ],
   "source": [
    "max_len_src = 0\n",
    "max_len_tgt = 0\n",
    "\n",
    "for item in raw_dataset:\n",
    "    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "    max_len_src = max(max_len_src, len(src_ids))\n",
    "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "print(f'Max length of source sentence: {max_len_src}')\n",
    "print(f'Max length of target sentence: {max_len_tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:53.594240Z",
     "iopub.status.busy": "2024-12-08T07:42:53.593456Z",
     "iopub.status.idle": "2024-12-08T07:42:53.598947Z",
     "shell.execute_reply": "2024-12-08T07:42:53.598162Z",
     "shell.execute_reply.started": "2024-12-08T07:42:53.594198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:53.600293Z",
     "iopub.status.busy": "2024-12-08T07:42:53.599966Z",
     "iopub.status.idle": "2024-12-08T07:42:53.636931Z",
     "shell.execute_reply": "2024-12-08T07:42:53.636025Z",
     "shell.execute_reply.started": "2024-12-08T07:42:53.600258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape->: torch.Size([8, 350])\n",
      "Decoder Input Shape->: torch.Size([8, 350])\n",
      "Label Shape->: torch.Size([8, 350])\n",
      "Encoder Mask Shape->: torch.Size([8, 1, 1, 350])\n",
      "Decoder Mask Shape->: torch.Size([8, 1, 350, 350])\n",
      "Source Text(en)->: ['A prince ought to have no other aim or thought, nor select anything else for his study, than war and its rules and discipline; for this is the sole art that belongs to him who rules, and it is of such force that it not only upholds those who are born princes, but it often enables men to rise from a private station to that rank. And, on the contrary, it is seen that when princes have thought more of ease than of arms they have lost their states.', 'He was on a thoroughbred dark bay, which was obviously heated by galloping, and he was using the reins to hold it in.', \"Of the fanatic's burning eternity I have no fear: there is not a future state worse than this present one--let me break away, and go home to God!'\", 'And as this point is worthy of notice, and to be imitated by others, I am not willing to leave it out.', \"'I am fond of the country,' said Vronsky, noticing, but pretending not to notice, Levin's tone.\", 'And the clergyman, who had not lifted his eyes from his book, and had held his breath but for a moment, was proceeding: his hand was already stretched towards Mr. Rochester, as his lips unclosed to ask, \"Wilt thou have this woman for thy wedded wife?\"--when a distinct and near voice said--', 'See that she is cared for as her condition demands, and you have done all that God and humanity require of you.', 'He, too, has always been called careful.']\n",
      "Target Text(it)->: [\"Debbe adunque uno principe non avere altro obietto né altro pensiero, né prendere cosa alcuna per sua arte, fuora della guerra et ordini e disciplina di essa; perché quella è sola arte che si espetta a chi comanda. Et è di tanta virtù, che non solamente mantiene quelli che sono nati principi, ma molte volte fa li uomini di privata fortuna salire a quel grado; e per avverso si vede che, quando e' principi hanno pensato più alle delicatezze che alle arme, hanno perso lo stato loro.\", 'Cavalcava un cavallo baio scuro, purosangue, evidentemente accaldatosi nel galoppo. Egli, nel trattenerlo, lavorava di redini.', 'Del fuoco eterno, inventato dai fanatici, non ho paura; non vi può essere vita futura peggiore della presente; spezziamo questa esistenza per tornare verso Dio, nella vera patria! —', 'E, perché questa parte è degna di notizia e da essere imitata da altri, non la voglio lasciare indrieto.', '— A me piace la campagna — disse Vronskij, avendo notato, ma fingendo di non rilevare, il tono di Levin.', 'Il pastore, che non aveva alzato gli occhi dal libro e non aveva trattenuto che un momento il respiro, stava per continuare; già la sua mano si era stesa verso il signor Rochester, già le labbra si schiudevano per domandare: \"Dichiarate di prendere questa ragazza per vostra legittima sposa?\" quando una voce chiara e distinta esclamò:', '\"Voglio che non le manchino le cure, e tu avrai fatto tutto ciò che esigono Iddio e l\\'umanità.', 'Anche lui è sempre stato economo.']\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(f\"Encoder Input Shape->: {batch['encoder_input'].shape}\")\n",
    "    print(f\"Decoder Input Shape->: {batch['decoder_input'].shape}\")\n",
    "    print(f\"Label Shape->: {batch['label'].shape}\")\n",
    "    print(f\"Encoder Mask Shape->: {batch['encoder_mask'].shape}\")\n",
    "    print(f\"Decoder Mask Shape->: {batch['decoder_mask'].shape}\")\n",
    "    print(f\"Source Text(en)->: {batch['src_text']}\")\n",
    "    print(f\"Target Text(it)->: {batch['tgt_text']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:53.638762Z",
     "iopub.status.busy": "2024-12-08T07:42:53.638403Z",
     "iopub.status.idle": "2024-12-08T07:42:53.672071Z",
     "shell.execute_reply": "2024-12-08T07:42:53.671284Z",
     "shell.execute_reply.started": "2024-12-08T07:42:53.638725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "def build_transformer(src_vocab_size: int, \n",
    "                      tgt_vocab_size: int, \n",
    "                      src_seq_len: int, \n",
    "                      tgt_seq_len: int, \n",
    "                      d_model: int=512,\n",
    "                      N: int=6,\n",
    "                      h: int=8,\n",
    "                      dropout: float=0.1,\n",
    "                      d_ff: int=2048) -> Transformer:\n",
    "    \n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:42:53.673449Z",
     "iopub.status.busy": "2024-12-08T07:42:53.673165Z",
     "iopub.status.idle": "2024-12-08T07:42:55.133302Z",
     "shell.execute_reply": "2024-12-08T07:42:55.132260Z",
     "shell.execute_reply.started": "2024-12-08T07:42:53.673421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-2): 3 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (src_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(15698, 512)\n",
       "  )\n",
       "  (tgt_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(22463, 512)\n",
       "  )\n",
       "  (src_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tgt_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (projection_layer): ProjectionLayer(\n",
       "    (proj): Linear(in_features=512, out_features=22463, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_src_len = tokenizer_src.get_vocab_size()\n",
    "vocab_tgt_len = tokenizer_tgt.get_vocab_size()\n",
    "\n",
    "model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T07:53:39.096029Z",
     "iopub.status.busy": "2024-12-08T07:53:39.095655Z",
     "iopub.status.idle": "2024-12-08T07:53:39.111000Z",
     "shell.execute_reply": "2024-12-08T07:53:39.110097Z",
     "shell.execute_reply.started": "2024-12-08T07:53:39.096000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model: build_transformer, \n",
    "                  source: torch.Tensor, \n",
    "                  source_mask: torch.Tensor,\n",
    "                  tokenizer_tgt: Tokenizer, \n",
    "                  max_len: int,\n",
    "                  device: torch.device)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Decodes a sequence from the source using a greedy decoding approach with a transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (\"build_transformer\"): The transformer model to be used for encoding and decoding.\n",
    "        source (torch.Tensor): The input tensor representing the source sequence. Shape: `(batch_size, seq_len)`.\n",
    "        source_mask (torch.Tensor): A mask for the source input sequence. Shape: `(batch_size, 1, seq_len)`.\n",
    "        tokenizer_tgt (\"Tokenizer\"): The tokenizer for the target language. Must provide `token_to_id` for special tokens.\n",
    "        max_len (int): The maximum length for the decoded sequence.\n",
    "        device (torch.device): The device (CPU or GPU) to perform the decoding on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the decoded sequence, excluding padding. Shape: `(seq_len,)`.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> import torch\n",
    "        >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        >>> src_tensor = torch.tensor([[1, 2, 3, 4]], dtype=torch.int64)\n",
    "        >>> src_mask = torch.tensor([[[1, 1, 1, 1]]], dtype=torch.int64)\n",
    "        >>> tgt_tokenizer = Tokenizer.from_file(\"tokenizer_tgt.json\")\n",
    "        >>> transformer_model = build_transformer()  # Example transformer model\n",
    "        >>> decoded_seq = greedy_decode(transformer_model, src_tensor, src_mask, tgt_tokenizer, max_len=20, device=device)\n",
    "        >>> print(decoded_seq)\n",
    "    \"\"\"\n",
    "    \n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def eval_model(model: build_tokenizer,\n",
    "               greedy_decode: callable,\n",
    "               val_dataloader: torch.utils.data.DataLoader, \n",
    "               tokenizer_src: Tokenizer, \n",
    "               tokenizer_tgt: Tokenizer,\n",
    "               max_len: int, \n",
    "               device: torch.device, \n",
    "               num_examples:int=2) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates a transformer model by generating predictions on a validation dataset using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model (\"build_transformer\"): The transformer model to be evaluated.\n",
    "        greedy_decode (callable): The function for performing greedy decoding on the model.\n",
    "        val_dataloader (torch.utils.data.DataLoader): The DataLoader for the validation dataset.\n",
    "        tokenizer_src (\"Tokenizer\"): Tokenizer for the source language.\n",
    "        tokenizer_tgt (\"Tokenizer\"): Tokenizer for the target language.\n",
    "        max_len (int): Maximum length for the decoded sequence.\n",
    "        device (torch.device): The device (CPU or GPU) to run the evaluation on.\n",
    "        num_examples (int, optional): Number of examples to print during evaluation. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the source text, expected target text, and predicted output to the console.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> from torch.utils.data import DataLoader\n",
    "        >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        >>> val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "        >>> src_tokenizer = Tokenizer.from_file(\"tokenizer_src.json\")\n",
    "        >>> tgt_tokenizer = Tokenizer.from_file(\"tokenizer_tgt.json\")\n",
    "        >>> eval_model(transformer_model, greedy_decode, val_loader, src_tokenizer, tgt_tokenizer, max_len=50, device=device)\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print('-'*console_width)\n",
    "            print(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print('-'*console_width)\n",
    "                break\n",
    "\n",
    "def translate_english_to_italian(model:build_transformer,\n",
    "                                 tokenizer_src: Tokenizer,\n",
    "                                 tokenizer_tgt: Tokenizer,\n",
    "                                 english_sentence: str,\n",
    "                                 max_len: int,\n",
    "                                 device: torch.device) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Italian using a trained transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (\"build_transformer\"): The trained transformer model.\n",
    "        tokenizer_src (\"Tokenizer\"): Tokenizer for the source language (English).\n",
    "        tokenizer_tgt (\"Tokenizer\"): Tokenizer for the target language (Italian).\n",
    "        english_sentence (str): The English sentence to translate.\n",
    "        max_len (int): Maximum length for the translated sentence.\n",
    "        device (torch.device): Device (CPU or GPU) to use for the translation.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated sentence in Italian.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the English sentence\n",
    "        src_tokens = tokenizer_src.encode(english_sentence).ids\n",
    "        src_tensor = torch.tensor([src_tokens], dtype=torch.int64).to(device)\n",
    "        \n",
    "        # Create a source mask\n",
    "        src_mask = (src_tensor != tokenizer_src.token_to_id(\"[PAD]\")).unsqueeze(1).unsqueeze(1).to(device)\n",
    "\n",
    "        # Perform greedy decoding to get the Italian translation\n",
    "        translated_tokens = greedy_decode(\n",
    "            model=model,\n",
    "            source=src_tensor,\n",
    "            source_mask=src_mask,\n",
    "            tokenizer_tgt=tokenizer_tgt,\n",
    "            max_len=max_len,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Decode the translated tokens to get the Italian sentence\n",
    "        italian_sentence = tokenizer_tgt.decode(translated_tokens.detach().cpu().numpy())\n",
    "    \n",
    "    return italian_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:02:43.227980Z",
     "iopub.status.busy": "2024-12-08T08:02:43.227444Z",
     "iopub.status.idle": "2024-12-08T11:18:23.179903Z",
     "shell.execute_reply": "2024-12-08T11:18:23.178868Z",
     "shell.execute_reply.started": "2024-12-08T08:02:43.227947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: Tesla P100-PCIE-16GB\n",
      "Device memory: 15.887939453125 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 2365/2365 [09:47<00:00,  4.03it/s, loss=6.550]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: CHAPTER XII\n",
      "    TARGET: XII\n",
      " PREDICTED: \n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The doctor answered him, and then began talking about the scenes in the city Duma.\n",
      "    TARGET: Il dottore aveva risposto, ma poi s’era messo a parlare sui disordini del consiglio di stato.\n",
      " PREDICTED: Il suo momento , e si , e il suo suo suo suo suo suo suo suo suoi .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Ma non è ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 2365/2365 [09:46<00:00,  4.04it/s, loss=5.231]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'They say that one who has been best man more than ten times never marries, and I wanted to be one for the tenth time to make myself safe, but was too late,' Count Sinyavin was saying to the pretty young Princess Charskaya, who had designs on him.\n",
      "    TARGET: — Dicono che non si sposa chi fa da compare d’anello più di dieci volte, e io volevo farlo per la decima volta per acquietarmi, ma il posto era occupato — diceva il conte Sinjavin alla graziosa principessa carskaja, che aveva delle mire su di lui.\n",
      " PREDICTED: — E che non è mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai , ma non è mai mai mai mai mai mai mai mai , ma non è mai , e che non si , e che non si , e che non si , e che non si , e che non si .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I think he was swearing, but am not certain; however, he was pronouncing some formula which prevented him from replying to me directly.\n",
      "    TARGET: Mi parve che bestemmiasse, ma non ne son sicura; però brontolò qualche parola che gl'impedì di rispondermi subito.\n",
      " PREDICTED: Io non ho detto che non ho detto che non mi , ma mi a me .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: — Che cosa ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=4.038]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There was a pause, and the mother exchanged glances with her daughter.\n",
      "    TARGET: Seguì un silenzio. La madre e la figlia si guardarono ancora una volta.\n",
      " PREDICTED: Era un ’ altra cosa , e la moglie , si mise a lei .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He saw that she was saying what she forced herself to utter and not what she wished to say.\n",
      "    TARGET: Egli sentiva che Anna diceva quello che s’era imposta di dire, non quello che avrebbe voluto dire.\n",
      " PREDICTED: Egli era un ’ amore , ma non si poteva nulla , e non poteva nulla .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Jane ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=6.192]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I have a vague recollection of having been woke up at least a dozen times during the night by Harris wandering about the boat with the lantern, looking for his clothes.\n",
      "    TARGET: Ho un vago ricordo d’essermi svegliato almeno una dozzina di volte durante la notte, per colpa di Harris che andava in giro nella barca con la lanterna, cercando i suoi panni.\n",
      " PREDICTED: Io mi piace un po ’ di nuovo , che non si , il fiume , con la chiusa di , e il sole , il sole .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And George said: \"Not at all;\" that it was his fault; and Harris said no, it was his.\n",
      "    TARGET: E Giorgio disse: — No, niente, non è colpa tua; — e Harris osservò che infatti, era sua.\n",
      " PREDICTED: E Giorgio disse : “ Non c ’ era nulla , ma non si mise a parlare , e non si mise a parlare a lei .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Che cosa siete ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=4.588]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'No, not now, later!' he said.\n",
      "    TARGET: — No, non ora, dopo! — egli disse.\n",
      " PREDICTED: — No , non ne parliamo più .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was very bitter, and he felt ashamed; yet mixed with the bitterness and the shame he felt a sense of joy and emotion at the greatness of his own humility.\n",
      "    TARGET: Quanta amarezza, quanta vergogna! ma insieme con quest’amarezza e con questa vergogna erano in lui la gioia e la commozione che gli venivano dall’altezza della propria umiltà.\n",
      " PREDICTED: Era così , e , senza rispondere , ma , senza , e , con la sua vita , e la sua vita , la sua felicità , la propria felicità .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Perché volete ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 2365/2365 [09:46<00:00,  4.04it/s, loss=2.985]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I think I'll take him,' replied Vronsky.\n",
      "    TARGET: — Credo che lo comprerò — rispose Vronskij.\n",
      " PREDICTED: — Io penso — disse Vronskij .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Ignorant midwives murder the babies, and the people remain steeped in ignorance, at the mercy of every village clerk; while you have in your power the means of helping them, and yet are not helping because you do not consider it important!'\n",
      "    TARGET: Queste mammane fanno morir di fame i bambini e il popolo marcisce nell’ignoranza e rimane in potere di un qualsiasi scribacchino, mentre tu hai in mano i mezzi per riparare a questo, e non te ne dài pensiero perché, secondo te, la cosa non è importante.\n",
      " PREDICTED: e i nostri , e i nostri tempi , in campagna , in campagna , in campagna , non si , e non si , e non si .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=4.089]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Karenin halted and turned pale.\n",
      "    TARGET: Aleksej Aleksandrovic si fermò e impallidì.\n",
      " PREDICTED: Aleksej Aleksandrovic si alzò e si alzò le spalle .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It's true Alesha did not stand very well: he kept turning round to see the back of his jacket; but nevertheless he was wonderfully sweet.\n",
      "    TARGET: È vero che Alëša non stava proprio del tutto composto: non faceva che voltarsi per rimirarsi il dietro del giubbetto; tuttavia era straordinariamente aggraziato.\n",
      " PREDICTED: È vero che non c ’ era un , ma , dopo aver guardato il cavallo , ma era un cavallo , e lei era un ’ altra cosa .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=4.391]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Accordingly, we went on board, took the arms which were left on board out of her, and whatever else we found there—which was a bottle of brandy, and another of rum, a few biscuit-cakes, a horn of powder, and a great lump of sugar in a piece of canvas (the sugar was five or six pounds): all which was very welcome to me, especially the brandy and sugar, of which I had had none left for many years.\n",
      "    TARGET: Detto fatto! Venuti alla scialuppa ne levammo l’armi che v’erano state lasciate entro, e quant’altre minuzie vi ritrovammo; cioè un fiaschetto d’acquavite, uno di rum, una piccola provvigione di biscotto, un fiaschetto di polvere, un gran pane di zucchero del peso di cinque libbre, avvolto in un pezzo di canovaccio, tutte cose capitate in buon punto per me, massime l’acquavite e lo zucchero, di cui non vedeva da molti anni il vestigio.\n",
      " PREDICTED: a bordo il vascello , e ci diede la terra , e ci diede la loro cosa per , e ci un ’ altra parte , un ’ altra estremità del mio moschetto , un ’ altra estremità , un ’ altra parte , un ’ altra parte , un ’ altra parte , un ’ altra parte , che non ne aveva mai avuto il tempo di , e il mio grano , nè un ’ altra parte , che non aveva avuto la nostra vita .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I walked fast through the room: I stopped, half suffocated with the thoughts that rose faster than I could receive, comprehend, settle them:--thoughts of what might, could, would, and should be, and that ere long. I looked at the blank wall: it seemed a sky thick with ascending stars,--every one lit me to a purpose or delight.\n",
      "    TARGET: Camminavo presto per la stanza, fermandomi ogni tanto nel sentirmi soffocare dai rapidi pensieri che mi sorgevano nella mente. Pensavo a tutto quello che poteva avvenire, che sarebbe avvenuto in breve; guardavo le pareti bianche e mi pareva di vederle coperte di un cielo tempestato di stelle; ognuna di quelle stelle mi guidava a una mèta deliziosa.\n",
      " PREDICTED: Mi sentivo poco in camera sua e mi accorsi che il suo spirito , che non potevo più nulla , che mi avrebbe potuto , e che mi parve che il rumore di una luce , e mi la notte , che mi un ' ora un ' ora e mi in una camera .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete mio figlio ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=3.850]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Oblonsky, half asleep, refused to budge so early.\n",
      "    TARGET: Oblonskij, nel sonno, si rifiutava di andare via così presto.\n",
      " PREDICTED: Stepan Arkad ’ ic , , , a lungo il tempo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Knowing that he is a kind and generous man – that I am not worth his little finger – nevertheless I hate him!\n",
      "    TARGET: Lo credi che, pur sapendo che egli è un uomo buono, eccellente, che io non valgo una sua unghia, tuttavia, io lo odio?\n",
      " PREDICTED: che è un uomo e non ho paura di un uomo che io non ho paura .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: \" Come siete mia amica ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=4.340]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs.\n",
      "    TARGET: Aguzzò gli occhi, e cercò di fissare il fondo, per scoprire qualche cosa; ma in fondo era buio pesto e non si scopriva nulla. Guardò le pareti del pozzo e s'accorse che erano rivestite di scaffali di biblioteche; e sparse qua e là di mappe e quadri, sospesi a chiodi.\n",
      " PREDICTED: In questo momento si , e si sentì che per lo stesso momento si ; ma ora era già in modo che , per lui si , si con gli occhi di un ’ altra parte , si , e si con gli occhi e si , si e si delle sue .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Come, Jane--come hither.\"\n",
      "    TARGET: — Venite, Jane, venite qui.\n",
      " PREDICTED: — Venite , Jane , Jane .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete mia figlia ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=2.924]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Oh dear, oh dear!' he muttered despairingly, as he recalled the most painful details of the quarrel.\n",
      "    TARGET: “Ahi, ahi, ahi!” ripeteva con disperazione, ricordando le impressioni più penose per lui di quella rottura.\n",
      " PREDICTED: — Ah , ahi , ahi ! — gridò lei , con chiarezza che le idee avevano espresso in mente .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Linen I had none left but what was mere rags; I had goat’s hair, but neither knew how to weave it or spin it; and had I known how, here were no tools to work it with.\n",
      "    TARGET: Biancheria io non ne avea che non fosse ridotta a veri cenci: aveva del pelo di capra; nè certo sapeva come si facesse nè a filarlo nè a tesserlo.\n",
      " PREDICTED: Non avevo mai potuto , ma che cosa era , perchè il collo non sapeva come se fosse , nè sapeva io avessi potuto ; e quanto io avessi potuto con tanta fatica , come io non aveva .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete mio amico ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=3.010]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The baby had taken the breast.\n",
      "    TARGET: Il bambino s’era attaccato al petto.\n",
      " PREDICTED: La bambina aveva le sue mani .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Do you know that our brother Nicholas is here again?'\n",
      "    TARGET: — Sai, Nikolaj è di nuovo qui.\n",
      " PREDICTED: — Sapete che il fratello è venuto a cominciare a cominciare a parlare ?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete mio amico ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=2.474]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: As if just awakened from a dream, it was long before he could collect his thoughts.\n",
      "    TARGET: Come se si fosse svegliato da un sonno, Levin stentò a tornare in sé.\n",
      " PREDICTED: Come se l ’ espressione di Pilato , il sogno era stato molto strano , era rimasto in tempo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"If I were you, I should cut down those limes, but it must be done when the sap rises.\n",
      "    TARGET: A criterio mio, codesto bosco di tigli, lo taglierei. Basta farlo quando è in succhio.\n",
      " PREDICTED: — Se vi , vi a questi alberi , ma credo che si a pregare , .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete mio amico ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=2.722]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Voila, Monsieur Rochester, qui revient!\"\n",
      "    TARGET: — Ecco il signor Rochester!\n",
      " PREDICTED: — , signorina , buona febbre .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I then returned: \"You are not without sense, cousin Eliza; but what you have, I suppose, in another year will be walled up alive in a French convent.\n",
      "    TARGET: — Voi pure non ne mancate, Elisa, ma quando penso che fra un anno il vostro buon senso vi avrà rinchiuso fra le mura di un convento francese.... del resto queste cose non mi riguardano, e se vi conviene, basta.\n",
      " PREDICTED: \" Allora mi avete detto , non siete accorto e neppure senza che avete avuto un vecchio , ma che vi ho visto come un giovane come una .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come avete fatto il mio amico ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=3.399]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: All that night and morning Levin had lived quite unconsciously, and felt quite outside the conditions of material existence.\n",
      "    TARGET: Tutta quella notte e la mattina seguente Levin aveva vissuto inconsciamente e si era sentito del tutto fuori della vita materiale.\n",
      " PREDICTED: Tutta la notte e Levin aveva passato il pensiero di Levin , sempre particolarmente tranquillo e chiara della gente .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Mrs. Fairfax will smile you a calm welcome, to be sure,\" said I; \"and little Adele will clap her hands and jump to see you: but you know very well you are thinking of another than they, and that he is not thinking of you.\"\n",
      "    TARGET: — La signora Fairfax, — dicevo a me stessa, — ti darà, sorridendo dolcemente, il benvenuto, Adele batterà le mani e ti salterà incontro, ma tu sai che pensi a un'altra persona e che quella persona non pensa a te!\n",
      " PREDICTED: — La signora Fairfax , il vostro sorriso , il mio sorriso vi ha detto , — disse Adele , e le mani per non sapere quello che dice ; ma ha fatto di dire che si tratta e non ha bisogno di nulla .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come state ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=3.794]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Nicholas had heard, but the words had no effect on him; his look remained reproachful and strained.\n",
      "    TARGET: Nikolaj aveva sentito; ma queste parole non produssero nessuna impressione su di lui. Il suo sguardo era sempre teso e accusatore.\n",
      " PREDICTED: Nikolaj era sorpreso , ma non aveva avuto il pensiero che gli era uscito , e si era messo a gridare a un tratto a un tratto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: At that moment Agatha Mikhaylovna came in with some jam.\n",
      "    TARGET: In quel momento entrò Agaf’ja Michajlovna con la marmellata.\n",
      " PREDICTED: In quel momento , in quel momento , con un certo prezzo .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete amica !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 2365/2365 [09:46<00:00,  4.03it/s, loss=2.796]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"You did right to hold fast to each other,\" I said: as if the monster- splinters were living things, and could hear me.\n",
      "    TARGET: — Fate bene di tenervi unite ancora, — dissi alle due parti dell'albero, come se potessero ascoltarmi.\n",
      " PREDICTED: — Mi rispose , — vi , — risposi , — anche se vi le mie forze .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But he was suddenly struck by the calm and cheerful expression of Katavasov's face, and felt so sorry to lose the spiritual condition which he was evidently spoiling by his conversation, that recollecting his resolution he ceased speaking.\n",
      "    TARGET: Ma l’espressione calma e allegra del viso di Katavasov lo colpì, a un tratto, e gli venne pietà dello stato d’animo proprio, che, evidentemente, egli turbava con quella conversazione; si ricordò del suo proposito e si fermò.\n",
      " PREDICTED: Ma Levin era in un tratto di spavento , allegra e chiaro , il suo viso , e sentiva così forte come il corpo ch ’ egli non poteva desiderare , che solo non era scontento di lui , ma che provava lui stesso piano , si sentiva di nuovo involontariamente .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come state il mio amico ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=2.425]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He also said he was ready for bed.\n",
      "    TARGET: Aggiunse che era pronto per andare a letto.\n",
      " PREDICTED: E tutto andava dicendo che il letto era pronto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You follow me, sur.\"\n",
      "    TARGET: Seguitemi, signore.\n",
      " PREDICTED: Mi .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come state ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=3.053]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"What foreign country was he going to, Bessie?\"\n",
      "    TARGET: — E in qual paese andava, Bessie?\n",
      " PREDICTED: — Che c ' è di partire , Bessie ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: VRONSKY DID NOT EVEN TRY TO SLEEP that night.\n",
      "    TARGET: Per tutta quella notte Vronskij non tentò neppure d’addormentarsi.\n",
      " PREDICTED: Vronskij non conosceva né quella notte .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come mai , amica mia ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=2.027]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And this consciousness was at first so painful, the fear lest that helpless being should suffer was so strong, that it quite hid the strange feeling of unreasoning joy and even pride which he experienced when the baby sneezed.\n",
      "    TARGET: E questa coscienza era così tormentosa nei primi tempi, il terrore che quell’essere impotente soffrisse era così forte, che proprio per questo non avvertiva lo strano sentimento di spensierata gioia e perfino di orgoglio ch’egli aveva provato proprio nel momento in cui il bambino aveva starnutito.\n",
      " PREDICTED: E la coscienza era così facile il più piccolo , che per essere costretto a , così come se il terrore avesse fatto la umiliazione a quell ’ incubo opprimente della propria umiliazione .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Therefore goodness is beyond the chain of cause and effect.\n",
      "    TARGET: Perciò, il bene è al di fuori della catena delle cause e degli effetti.\n",
      " PREDICTED: E per quel mistero è l ’ unico più difficile per avere la loro disgrazia .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come , amica mia ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(model: build_tokenizer,\n",
    "                greedy_decode: callable,\n",
    "                tokenizer_src:Tokenizer,\n",
    "                tokenizer_tgt:Tokenizer,\n",
    "                val_dataloader: torch.utils.data.DataLoader,\n",
    "                train_dataloader: torch.utils.data.DataLoader,\n",
    "                optimizer: torch.optim,\n",
    "                loss_fn: nn.CrossEntropyLoss,\n",
    "                english_sentence:str,\n",
    "                config: dict, \n",
    "                device: torch.device) -> None:\n",
    "    \"\"\"\n",
    "    Trains a transformer model for a sequence-to-sequence task using the provided dataset and configuration.\n",
    "\n",
    "    Args:\n",
    "        model (\"build_transformer\"): The transformer model to train.\n",
    "        greedy_decode (callable): Function for greedy decoding during validation.\n",
    "        tokenizer_src (\"Tokenizer\"): Tokenizer for the source language.\n",
    "        tokenizer_tgt (\"Tokenizer\"): Tokenizer for the target language.\n",
    "        val_dataloader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "        train_dataloader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        loss_fn (nn.CrossEntropyLoss): Loss function for computing the training loss.\n",
    "        config (dict): Configuration dictionary containing hyperparameters such as `num_epochs` and `seq_len`.\n",
    "        device (torch.device): Device (CPU or GPU) to use for training.\n",
    "\n",
    "    Returns:\n",
    "        None: The function trains the model in-place and prints progress and evaluation results after each epoch.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> from torch.utils.data import DataLoader\n",
    "        >>> import torch.nn as nn\n",
    "        >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        >>> config = {\"num_epochs\": 10, \"seq_len\": 50}\n",
    "        >>> train_model(\n",
    "        ...     model=transformer_model,\n",
    "        ...     greedy_decode=greedy_decode,\n",
    "        ...     tokenizer_src=src_tokenizer,\n",
    "        ...     tokenizer_tgt=tgt_tokenizer,\n",
    "        ...     val_dataloader=val_loader,\n",
    "        ...     train_dataloader=train_loader,\n",
    "        ...     optimizer=torch.optim.Adam(transformer_model.parameters(), lr=0.001),\n",
    "        ...     loss_fn=nn.CrossEntropyLoss(),\n",
    "        ...     config=config,\n",
    "        ...     device=device,\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "\n",
    "    initial_epoch = 0\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            \n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        eval_model(model, greedy_decode ,val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device)\n",
    "        \n",
    "        predicted_italian_sentence = translate_english_to_italian(\n",
    "                model=model,\n",
    "                tokenizer_src=tokenizer_src,\n",
    "                tokenizer_tgt=tokenizer_tgt,\n",
    "                english_sentence=english_sentence,\n",
    "                max_len=50,\n",
    "                device=device,\n",
    "            )\n",
    "        \n",
    "        print()\n",
    "        print(\"-\"*150)\n",
    "        print(f\"English Sentence: {english_sentence}\")\n",
    "        print(f\"Actual Italian Sentence: {'come stai amico mio?'}\")\n",
    "        print(f\"Predicted Italian Sentence: {predicted_italian_sentence}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "    english_sentence = \"how are you my friend?\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    train_model(\n",
    "            model=model,\n",
    "            greedy_decode=greedy_decode,\n",
    "            tokenizer_src=tokenizer_src,\n",
    "            tokenizer_tgt=tokenizer_tgt,\n",
    "            val_dataloader=val_dataloader,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            english_sentence=english_sentence,\n",
    "            config=config,\n",
    "            device=device,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T11:20:16.391591Z",
     "iopub.status.busy": "2024-12-08T11:20:16.390750Z",
     "iopub.status.idle": "2024-12-08T11:20:16.396189Z",
     "shell.execute_reply": "2024-12-08T11:20:16.395277Z",
     "shell.execute_reply.started": "2024-12-08T11:20:16.391556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 6,\n",
    "        \"lr\": 10**-3,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T11:20:45.148531Z",
     "iopub.status.busy": "2024-12-08T11:20:45.148197Z",
     "iopub.status.idle": "2024-12-08T12:19:22.152112Z",
     "shell.execute_reply": "2024-12-08T12:19:22.151226Z",
     "shell.execute_reply.started": "2024-12-08T11:20:45.148502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: Tesla P100-PCIE-16GB\n",
      "Device memory: 15.887939453125 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=2.246]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"No.\"\n",
      "    TARGET: — No.\n",
      " PREDICTED: — No .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Should any little accidental disappointment of the appetite occur, such as the spoiling of a meal, the under or the over dressing of a dish, the incident ought not to be neutralised by replacing with something more delicate the comfort lost, thus pampering the body and obviating the aim of this institution; it ought to be improved to the spiritual edification of the pupils, by encouraging them to evince fortitude under temporary privation.\n",
      "    TARGET: Se accade loro un piccolo incidente, un pasto guastato, per esempio, non si deve paralizzare l'effetto dell'azione. Voi dimenticate lo scopo di questa istituzione e certi avvenimenti dovrebbero esser cagione di edificazione per le alunne; sarebbe quello il momento di predicare la forza d'animo nelle privazioni della vita, e un saggio educatore dovrebbe trarne argomento per rammentare le sofferenze dei primi cristiani, il tormento dei martiri, l'esempio del Divin Maestro.\n",
      " PREDICTED: Se si dovesse crescere a quella disposizione simile a quella faccenda , almeno sulla quale durò una cena , non è ancora neppure neppure una risposta a quella del carbone con quella dolce , con la quale li faceva ogni tanto più facile ; perché la quale la quale la quale la straordinaria impressione delle cose sarebbero rivolte per la tendenza all ' opera .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come , amico mio ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=1.926]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Turn that Dormouse out of court!\n",
      "    TARGET: Fuori quel Ghiro!\n",
      " PREDICTED: il Ghiro !\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: What would happen to her and to her son, toward whom his feelings had changed as they had toward her, no longer occupied his mind.\n",
      "    TARGET: Tutto quello che sarebbe poi accaduto di lei e del figlio, verso il quale, così come verso di lei, si eran mutati i suoi sentimenti, non lo interessava più.\n",
      " PREDICTED: Ma cosa , per quanto avrebbe potuto amare il figlio , in cui era accaduto fra i suoi sentimenti , non era più nulla di prima .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come mai hai torto ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=1.543]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I did not care to go out of sight of the boat, fearing the coming of canoes with savages down the river; but the boy seeing a low place about a mile up the country, rambled to it, and by-and-by I saw him come running towards me.\n",
      "    TARGET: Non mi piacea di perdere di vista la scialuppa, per paura che alcuni canotti di selvaggi scendessero lungo il fiume; ma il ragazzo scorgendo una valletta lontana circa un miglio dal luogo ove eravamo, si trasse fin là, nè andò guari che il vidi tornare a me correndo come il vento. Pensai fosse inseguìto da qualche uomo, o spaventato da qualche fiera, onde gli corsi incontro per aiutarlo; ma quando gli fui più vicino, vidi alcun che pendergli dalle spalle.\n",
      " PREDICTED: Non m ’ importa d ’ esser venuta a vedere la vista di vedere veder la barca , che , verso il pesce , aveva una specie di metri lontano dal bosco , e in un paese , mi sentii che lontano .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And the conversation turned upon the topic that interested Dolly more than any other: confinements, children's illnesses, husbands' whereabouts, and whether they came home often.\n",
      "    TARGET: E la conversazione divenne quanto mai interessante per Dar’ja Aleksandrovna: come aveva partorito? di che cosa s’era ammalata? dov’era il marito? veniva spesso?\n",
      " PREDICTED: E la conversazione si mise allo stesso conversazione che diceva Dar ’ ja Aleksandrovna : tutti i bambini e i bambini di tutti i bambini , e i più piccoli , se fossero rimasti in casa .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come state ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 2365/2365 [09:44<00:00,  4.04it/s, loss=2.030]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I speak particularly of the young ladies. St. John's eyes, though clear enough in a literal sense, in a figurative one were difficult to fathom.\n",
      "    TARGET: Parlo particolarmente delle due ragazze, perché gli occhi di Saint-John, benché fossero chiari nel significato vero della parola, erano inesplorabili.\n",
      " PREDICTED: Io parlo sinceramente , tutti i signori di Saint - John , ma quella differenza che amava una spiegazione , anche qualunque fosse il pensiero diversa da una cosa più vicina , che amava .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He went down to see what it was.\n",
      "    TARGET: Stepan Arkad’ic uscì a vedere.\n",
      " PREDICTED: di vedere quello che era avvenuto .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come state ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=1.950]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The wind sighed low in the firs: all was moorland loneliness and midnight hush.\n",
      "    TARGET: Il vento stormiva dolcemente fra gli abeti; intorno a me non vidi altro che solitudine.\n",
      " PREDICTED: La violenza , il vento più mite s ' era di riposo e di gliene aveva .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But the time came when I understood that I could no longer deceive myself that I am alive, and cannot be blamed because God made me so, that I want to love and to live.\n",
      "    TARGET: Ma è venuto poi il momento in cui ho compreso, in cui non mi è stato più possibile ingannare me stessa, in cui ho sentito che ero viva, che non avevo colpa se Dio mi aveva fatto così per l’amore e per la vita.\n",
      " PREDICTED: Ma , dopo che non son venuto più a lungo , non posso vivere in che a Dio non posso vivere , e a Dio mi sarei felice la felicità che l ’ amore e io voglio vivere .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Tu , come ho detto ?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 2365/2365 [09:45<00:00,  4.04it/s, loss=2.047]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She wore a white dress trimmed with wide embroidery, and as she sat in a corner of the verandah behind some plants, did not hear Vronsky coming.\n",
      "    TARGET: Vestiva un abito bianco con un largo ricamo; sedeva in un angolo della terrazza di là dai fiori e non aveva avvertito l’avvicinarsi di lui.\n",
      " PREDICTED: in un fuggevole vestito , e si preparava a guardare dietro , in un angolo , le finestre si fermò dinanzi a Vronskij che non portava le assi da Vronskij .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: From that day Montmorency regarded the kettle with a mixture of awe, suspicion, and hate.\n",
      "    TARGET: Da quel giorno Montmorency guardò il calderino con un misto di timore, di sospetto e di odio.\n",
      " PREDICTED: Dal giorno che Montmorency aveva un certo buon senso di rimprovero , senza ragione , e ne .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "English Sentence: how are you my friend?\n",
      "Actual Italian Sentence: come stai amico mio?\n",
      "Predicted Italian Sentence: Come siete mio amico ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "            model=model,\n",
    "            greedy_decode=greedy_decode,\n",
    "            tokenizer_src=tokenizer_src,\n",
    "            tokenizer_tgt=tokenizer_tgt,\n",
    "            val_dataloader=val_dataloader,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            english_sentence=english_sentence,\n",
    "            config=config,\n",
    "            device=device,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:24:13.422625Z",
     "iopub.status.busy": "2024-12-08T12:24:13.422054Z",
     "iopub.status.idle": "2024-12-08T12:24:13.432270Z",
     "shell.execute_reply": "2024-12-08T12:24:13.431371Z",
     "shell.execute_reply.started": "2024-12-08T12:24:13.422591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_bleu_score(\n",
    "                        model: build_transformer,\n",
    "                        dataloader: torch.utils.data.DataLoader,\n",
    "                        tokenizer_src: Tokenizer,\n",
    "                        tokenizer_tgt: Tokenizer,\n",
    "                        max_len: int,\n",
    "                        device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average BLEU score for a model on a given dataset with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        model (callable): The trained transformer model.\n",
    "        dataloader (torch.utils.data.DataLoader): Dataloader containing validation or test data.\n",
    "        tokenizer_src (Tokenizer): Tokenizer for the source language (English).\n",
    "        tokenizer_tgt (Tokenizer): Tokenizer for the target language (Italian).\n",
    "        max_len (int): Maximum length for the generated sentences.\n",
    "        device (torch.device): Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        float: The average BLEU score over the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_bleu_score = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=len(dataloader), desc=\"Calculating BLEU Score\", unit=\"batch\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                encoder_input = batch[\"encoder_input\"].to(device)\n",
    "                encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "                target_text = batch[\"tgt_text\"]  # List of actual Italian sentences\n",
    "                \n",
    "                # Generate the translation using the model\n",
    "                generated_tokens = translate_english_to_italian(\n",
    "                    model=model,\n",
    "                    tokenizer_src=tokenizer_src,\n",
    "                    tokenizer_tgt=tokenizer_tgt,\n",
    "                    english_sentence=batch[\"src_text\"][0],  # Assuming batch size = 1\n",
    "                    max_len=max_len,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                # Decode the generated tokens to text\n",
    "                generated_text = generated_tokens.split()  # Split into words\n",
    "                reference_text = [target_text[0].split()]  # Reference text as a list of words\n",
    "                \n",
    "                # Calculate BLEU score for this sentence\n",
    "                bleu_score = sentence_bleu(reference_text, generated_text)\n",
    "                total_bleu_score += bleu_score\n",
    "                count += 1\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.set_postfix({\"BLEU (running avg)\": f\"{(total_bleu_score / count):.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Calculate average BLEU score\n",
    "    average_bleu_score = total_bleu_score / count if count > 0 else 0.0\n",
    "    return average_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:25:53.741456Z",
     "iopub.status.busy": "2024-12-08T12:25:53.741119Z",
     "iopub.status.idle": "2024-12-08T12:43:18.955873Z",
     "shell.execute_reply": "2024-12-08T12:43:18.954960Z",
     "shell.execute_reply.started": "2024-12-08T12:25:53.741427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BLEU Score: 100%|██████████| 2365/2365 [09:14<00:00,  4.27batch/s, BLEU (running avg)=0.4594]\n",
      "Calculating BLEU Score: 100%|██████████| 2102/2102 [08:10<00:00,  4.28batch/s, BLEU (running avg)=0.4034]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Average BLEU Score: 0.4593545\n",
      "Val Average BLEU Score: 0.4034139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_train_bleu = calculate_bleu_score(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    tokenizer_src=tokenizer_src,\n",
    "    tokenizer_tgt=tokenizer_tgt,\n",
    "    max_len=config['seq_len'],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "average_val_bleu = calculate_bleu_score(\n",
    "    model=model,\n",
    "    dataloader=val_dataloader,\n",
    "    tokenizer_src=tokenizer_src,\n",
    "    tokenizer_tgt=tokenizer_tgt,\n",
    "    max_len=config['seq_len'],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Train Average BLEU Score: {average_train_bleu:.7f}\")\n",
    "print(f\"Val Average BLEU Score: {average_val_bleu:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T13:12:44.408688Z",
     "iopub.status.busy": "2024-12-08T13:12:44.408321Z",
     "iopub.status.idle": "2024-12-08T13:12:44.896940Z",
     "shell.execute_reply": "2024-12-08T13:12:44.896133Z",
     "shell.execute_reply.started": "2024-12-08T13:12:44.408659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'english_to_italian.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T13:16:17.649593Z",
     "iopub.status.busy": "2024-12-08T13:16:17.648904Z",
     "iopub.status.idle": "2024-12-08T13:16:17.890047Z",
     "shell.execute_reply": "2024-12-08T13:16:17.889164Z",
     "shell.execute_reply.started": "2024-12-08T13:16:17.649562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-2): 3 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (src_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(15698, 512)\n",
       "  )\n",
       "  (tgt_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(22463, 512)\n",
       "  )\n",
       "  (src_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tgt_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (projection_layer): ProjectionLayer(\n",
       "    (proj): Linear(in_features=512, out_features=22463, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('english_to_italian.pth'))\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
