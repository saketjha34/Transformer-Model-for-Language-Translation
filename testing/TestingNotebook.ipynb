{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from model import build_transformer\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset:load_dataset, language:str):\n",
    "    \"\"\"\n",
    "    Generator function that extracts all sentences in a specified language from a translation dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (load_dataset): A Hugging Face `load_dataset` object representing the dataset containing \n",
    "            translations in different languages.\n",
    "        language (str): The target language for which sentences are to be extracted. This should correspond \n",
    "            to a key in the `translation` field of the dataset.\n",
    "\n",
    "    Yields:\n",
    "        str: Sentences in the specified language from the dataset.\n",
    "\n",
    "    Example:\n",
    "        >>> from datasets import load_dataset\n",
    "        >>> dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
    "        >>> language = \"en\"\n",
    "        >>> sentences = get_all_sentences(dataset['train'], language)\n",
    "        >>> for sentence in list(sentences)[:5]:\n",
    "        ...     print(sentence)\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in dataset:\n",
    "        yield item['translation'][language]\n",
    "\n",
    "def build_tokenizer(config, dataset:load_dataset, language:str) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Builds or loads a tokenizer for a specified language using the Hugging Face `Tokenizers` library.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A configuration dictionary containing the `tokenizer_file` key, which specifies \n",
    "            the file path template for saving/loading the tokenizer. The file path should include a \n",
    "            placeholder for the language.\n",
    "        dataset (load_dataset): A Hugging Face `load_dataset` object representing the dataset containing \n",
    "            translations in different languages.\n",
    "        language (str): The target language for which the tokenizer is being built or loaded. This should \n",
    "            correspond to a key in the `translation` field of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: A `Tokenizer` object built for the specified language.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> from datasets import load_dataset\n",
    "        >>> config = {\"tokenizer_file\": \"tokenizer_{language}.json\"}\n",
    "        >>> dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
    "        >>> language = \"en\"\n",
    "        >>> tokenizer = build_tokenizer(config, dataset['train'], language)\n",
    "        >>> print(tokenizer.get_vocab_size())\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(language))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    print(f\"Tokenizer language : {language} Build Complete.\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saket\\AppData\\Local\\Temp\\ipykernel_44084\\872906554.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../pytorch pretrained model/english_to_italian.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-2): 3 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (src_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(15698, 512)\n",
       "  )\n",
       "  (tgt_embed): InputEmbeddings(\n",
       "    (embedding): Embedding(22463, 512)\n",
       "  )\n",
       "  (src_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tgt_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (projection_layer): ProjectionLayer(\n",
       "    (proj): Linear(in_features=512, out_features=22463, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src = Tokenizer.from_file(\"../tokenizers/tokenizer_en.json\")\n",
    "tokenizer_tgt = Tokenizer.from_file(\"../tokenizers/tokenizer_it.json\")\n",
    "\n",
    "\n",
    "vocab_src_len = tokenizer_src.get_vocab_size()\n",
    "vocab_tgt_len = tokenizer_tgt.get_vocab_size()\n",
    "seq_len = 350\n",
    "\n",
    "\n",
    "model = build_transformer(vocab_src_len, vocab_tgt_len, seq_len, seq_len,).to(device)\n",
    "model.load_state_dict(torch.load('../pytorch pretrained model/english_to_italian.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size:int)->bool:\n",
    "    \"\"\"\n",
    "    Creates a causal mask for autoregressive models.\n",
    "\n",
    "    Args:\n",
    "        size (int): The size of the square matrix for the mask.\n",
    "\n",
    "    Returns:\n",
    "        bool: A boolean tensor of shape (1, size, size), where the upper triangular part above the diagonal \n",
    "              is masked (False) and the rest is unmasked (True).\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> mask = causal_mask(5)\n",
    "        >>> print(mask)\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "\n",
    "def greedy_decode(model: build_transformer, \n",
    "                  source: torch.Tensor, \n",
    "                  source_mask: torch.Tensor,\n",
    "                  tokenizer_tgt: Tokenizer, \n",
    "                  max_len: int,\n",
    "                  device: torch.device)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Decodes a sequence from the source using a greedy decoding approach with a transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (\"build_transformer\"): The transformer model to be used for encoding and decoding.\n",
    "        source (torch.Tensor): The input tensor representing the source sequence. Shape: `(batch_size, seq_len)`.\n",
    "        source_mask (torch.Tensor): A mask for the source input sequence. Shape: `(batch_size, 1, seq_len)`.\n",
    "        tokenizer_tgt (\"Tokenizer\"): The tokenizer for the target language. Must provide `token_to_id` for special tokens.\n",
    "        max_len (int): The maximum length for the decoded sequence.\n",
    "        device (torch.device): The device (CPU or GPU) to perform the decoding on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the decoded sequence, excluding padding. Shape: `(seq_len,)`.\n",
    "\n",
    "    Example:\n",
    "        >>> from tokenizers import Tokenizer\n",
    "        >>> import torch\n",
    "        >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        >>> src_tensor = torch.tensor([[1, 2, 3, 4]], dtype=torch.int64)\n",
    "        >>> src_mask = torch.tensor([[[1, 1, 1, 1]]], dtype=torch.int64)\n",
    "        >>> tgt_tokenizer = Tokenizer.from_file(\"tokenizer_tgt.json\")\n",
    "        >>> transformer_model = build_transformer()  # Example transformer model\n",
    "        >>> decoded_seq = greedy_decode(transformer_model, src_tensor, src_mask, tgt_tokenizer, max_len=20, device=device)\n",
    "        >>> print(decoded_seq)\n",
    "    \"\"\"\n",
    "    \n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "def translate_english_to_italian(model:build_transformer,\n",
    "                                 tokenizer_src: Tokenizer,\n",
    "                                 tokenizer_tgt: Tokenizer,\n",
    "                                 english_sentence: str,\n",
    "                                 max_len: int,\n",
    "                                 device: torch.device) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Italian using a trained transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (\"build_transformer\"): The trained transformer model.\n",
    "        tokenizer_src (\"Tokenizer\"): Tokenizer for the source language (English).\n",
    "        tokenizer_tgt (\"Tokenizer\"): Tokenizer for the target language (Italian).\n",
    "        english_sentence (str): The English sentence to translate.\n",
    "        max_len (int): Maximum length for the translated sentence.\n",
    "        device (torch.device): Device (CPU or GPU) to use for the translation.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated sentence in Italian.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the English sentence\n",
    "        src_tokens = tokenizer_src.encode(english_sentence).ids\n",
    "        src_tensor = torch.tensor([src_tokens], dtype=torch.int64).to(device)\n",
    "        \n",
    "        # Create a source mask\n",
    "        src_mask = (src_tensor != tokenizer_src.token_to_id(\"[PAD]\")).unsqueeze(1).unsqueeze(1).to(device)\n",
    "\n",
    "        # Perform greedy decoding to get the Italian translation\n",
    "        translated_tokens = greedy_decode(\n",
    "            model=model,\n",
    "            source=src_tensor,\n",
    "            source_mask=src_mask,\n",
    "            tokenizer_tgt=tokenizer_tgt,\n",
    "            max_len=max_len,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Decode the translated tokens to get the Italian sentence\n",
    "        italian_sentence = tokenizer_tgt.decode(translated_tokens.detach().cpu().numpy())\n",
    "    \n",
    "    return italian_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence : How are you?\n",
      "Actual Italian Sentence : Come stai?\n",
      "Translated Italian Sentence : Come siete ?\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"How are you?\"\n",
    "italian_sentence = \"Come stai?\"\n",
    "predicted_italian_sentence = translate_english_to_italian(\n",
    "                                    model=model,\n",
    "                                    tokenizer_src=tokenizer_src,\n",
    "                                    tokenizer_tgt=tokenizer_tgt,\n",
    "                                    english_sentence=english_sentence,\n",
    "                                    max_len=350,\n",
    "                                    device=device)\n",
    "\n",
    "print(f\"English Sentence : {english_sentence}\")\n",
    "print(f\"Actual Italian Sentence : {italian_sentence}\")\n",
    "print(f\"Translated Italian Sentence : {predicted_italian_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
